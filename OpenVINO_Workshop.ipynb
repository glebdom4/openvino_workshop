{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks with OpenVINO: make them fly\n",
    "![](pictures/openvino_start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is inference and why do we need separate tools for it?\n",
    "\n",
    "![](pictures/training_vs_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is OpenVINO Toolkit anyway?\n",
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Full-featured pipeline underhood\n",
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first step of this workshop is initializing OpenVINO environment in this Jupyter notebook. \n",
    "The OpenVINO 2020.1 package have been installed to `intel/openvino/` already.\n",
    "For initializing the OpenVINO environment you should run the script `intel/openvino/bin/setupvars.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!bash ~/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Open Model Zoo\n",
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The OpenVINO package contains tools for easy download model from [OpenModelZoo](https://github.com/opencv/open_model_zoo) \n",
    "and convert the model to Intermediate Representation that OpenVINO supports\n",
    "\n",
    "To see all available models (both public open-sourse from original frameworks (TensorFlow, Caffe, MxNet, Pytorch e.t.c),\n",
    "and made in Intel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For downloading any of these models you need to use downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SSD-MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](pictures/mobileNet-SSD-network-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How SSD works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](pictures/ssd_boxes.png)\n",
    "\n",
    "![](pictures/ssd_loc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MobileNet_v2 review\n",
    "\n",
    "![](pictures/mobilenetv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try to download an object detection model `ssd_mobilenet_v2_coco`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!python3  ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py \\\n",
    "--name ssd_mobilenet_v2_coco \\\n",
    "--output_dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model Downloader downloaded the model to `data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!ls data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](pictures/openvino_support.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "But the Model Downloader downloaded the model in TensorFlow format.\n",
    "You need convert this model to IR format. \n",
    "For this you need run converter script\n",
    "converter script runs the Model Optimizer with right parameters to converting the model with to IR.\n",
    "Of course  we can run the Model Optimizer directly. But for this we need pass right arguments to the Model Optimizer.\n",
    "All information about converting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](pictures/model_optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Easy to use, Python*-based workflow does not require rebuilding frameworks.\n",
    "- Import Models from many supported frameworks: Caffe*, TensorFlow*, MXNet*, Kaldi*, exchange formats like ONNX* (Pytorch*, Caffe2* and others through ONNX).\n",
    "- 100+ models for Caffe, MXNet, TensorFlow validated. Supports all ONNX* model zoo public models.\n",
    "- Extends inferencing for non-vision networks with support of LSTM, Bert, GNMT, TDNN-LSTM, ESPNet and more.\n",
    "- IR files for models using standard layers or user-provided custom layers do not require Caffe.\n",
    "- Fallback to original framework is possible in cases of unsupported layers, but requires original framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](pictures/mo_result1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/converter.py \\\n",
    "--name ssd_mobilenet_v2_coco \\\n",
    "--download_dir ./data \\\n",
    "--output_dir ./data \\\n",
    "--precisions FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!ls data/public/ssd_mobilenet_v2_coco/FP32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can find a command of running OpenVINO Model Optimizer in the output of the converter.py script.\n",
    "You can try this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \\\n",
    "--output_dir=data/public/ssd_mobilenet_v2_coco/FP32 \\\n",
    "--reverse_input_channels \\\n",
    "--model_name=ssd_mobilenet_v2_coco \\\n",
    "--transformations_config=${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
    "--tensorflow_object_detection_api_pipeline_config=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\n",
    "--output=detection_classes,detection_scores,detection_boxes,num_detections \\\n",
    "--input_model=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Needed python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import OpenCV for image processing\n",
    "import cv2\n",
    "\n",
    "# Import some functions from matplotlib for show an image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Import needed functions from TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "# Import OpenVINO Inference Engine classes\n",
    "from openvino.inference_engine import IENetwork, IEPlugin, IECore\n",
    "\n",
    "# Import other needed functions\n",
    "import numpy as np\n",
    "\n",
    "import logging as log\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some function are needed for the next part of the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def read_resize_image(path_to_image: str, width: int, height: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes an image and resizes it to the given dimensions\n",
    "    \"\"\"\n",
    "    #Load the image \n",
    "    raw_image = cv2.imread(path_to_image)\n",
    "    #Return the resized to (width, height) size image  \n",
    "    return cv2.resize(raw_image, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_performance(performance_data: dict):\n",
    "    \"\"\"\n",
    "    Takes dictionary contains name of configurations as keys and FPS for it as values\n",
    "    Plots bar chart with data\n",
    "    \"\"\"\n",
    "    l = np.arange(len(performance_data))\n",
    "    \n",
    "    performance = [fps for _, fps in performance_data.items()]\n",
    "    configurations = list(performance_data.keys())\n",
    "    figsize=(3*len(performance_data),10)\n",
    "    print(figsize)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    bars = ax.bar(x=l, height=performance, tick_label=configurations)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('#DDDDDD')\n",
    "    \n",
    "    ax.tick_params(bottom=False, left=False)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(True, color='#EEEEEE')\n",
    "    ax.xaxis.grid(False)\n",
    "   \n",
    "    bar_color = bars[0].get_facecolor()\n",
    "\n",
    "    for bar in bars:\n",
    "      ax.text(\n",
    "          bar.get_x() + bar.get_width() / 2,\n",
    "          bar.get_height() + 5,\n",
    "          round(bar.get_height(), 1),\n",
    "          horizontalalignment='center',\n",
    "          color=bar_color,\n",
    "          weight='bold',\n",
    "          fontsize=17\n",
    "      )\n",
    "    ax.set_xlabel('Configurations', labelpad=15, color='#333333')\n",
    "    ax.set_ylabel('Frame per seconds', labelpad=15, color='#333333')\n",
    "    ax.set_title('Performance mesuarments', pad=15, color='#333333', weight='bold')\n",
    "    plt.ylim(0, max(performance)+20)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def draw_image(original_image: str,\n",
    "               res: tuple,\n",
    "               path_to_image: str,\n",
    "               prob_threshold: float=0.8,\n",
    "               color: tuple=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Takes a path to the image and bounding boxes. Draws those boxes on the new image and saves it\n",
    "    \"\"\"\n",
    "    raw_image = cv2.imread(original_image)\n",
    "    initial_w = raw_image.shape[1]\n",
    "    initial_h = raw_image.shape[0]\n",
    "    labels_map = {\n",
    "        18: 'dog',\n",
    "        21: 'cat'\n",
    "    }\n",
    "    for obj in res[0][0]:\n",
    "        # Draw only objects when probability more than specified threshold\n",
    "        if obj[2] > prob_threshold:\n",
    "            xmin = int(obj[3] * initial_w)\n",
    "            ymin = int(obj[4] * initial_h)\n",
    "            xmax = int(obj[5] * initial_w)\n",
    "            ymax = int(obj[6] * initial_h)\n",
    "            class_id = int(obj[1])\n",
    "            confidence = round(obj[2] * 100, 1)\n",
    "            cv2.rectangle(raw_image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            det_label = labels_map[class_id] if labels_map else str(class_id)\n",
    "            box_title = '{} {}%'.format(det_label, confidence)\n",
    "            cv2.putText(raw_image,\n",
    "                        box_title,\n",
    "                        (xmin, ymin - 7),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 5, color, cv2.LINE_AA)\n",
    "    cv2.imwrite(path_to_image, raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_results_interactively(tf_image: str, ie_image: str, combination_image: str, ie_fps:float, tf_fps:float):\n",
    "    \"\"\"\n",
    "    Takes paths to three images and shows them with matplotlib on one screen\n",
    "    \"\"\"\n",
    "    _ = plt.figure(figsize=(30, 10))\n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(wspace=0.25, hspace=0.05)\n",
    "\n",
    "    titles = [\n",
    "        '(a) Tensorflow',\n",
    "        '(b) Inference Engine',\n",
    "        '(c) TensorFlow and Inference Engine\\n predictions are identical'\n",
    "    ]\n",
    "\n",
    "    for i, path in enumerate([tf_image, ie_image, combination_image]):\n",
    "        img_resized = cv2.imread(path)\n",
    "        ax_plot = plt.subplot(gs1[i])\n",
    "        ax_plot.axis(\"off\")\n",
    "        addon = ' '\n",
    "        if i == 1:\n",
    "            addon += '{:4.3f}'.format(ie_fps) + '(FPS)'\n",
    "        elif i == 0:\n",
    "            addon += '{:4.3f}'.format(tf_fps) + '(FPS)'\n",
    "\n",
    "        ax_plot.text(0.5, -0.5, titles[i] + addon,\n",
    "                     size=28, ha=\"center\",\n",
    "                     transform=ax_plot.transAxes)\n",
    "        ax_plot.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def load_graph(path_to_model: str):\n",
    "    \"\"\"\n",
    "    Creates in memory graph in TensorFlow\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(path_to_model, \"rb\") as model_file:\n",
    "        graph_def.ParseFromString(model_file.read())\n",
    "\n",
    "    nodes_to_clear_device = graph_def.node if isinstance(\n",
    "        graph_def, tf.GraphDef) else graph_def.graph_def.node\n",
    "    for node in nodes_to_clear_device:\n",
    "        node.device = \"\"\n",
    "\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    log.info(\"tf graph was created\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def children(op_name: str, graph: tf.Graph):\n",
    "    \"\"\"\n",
    "    Get operation node children\n",
    "    \"\"\"\n",
    "    op = graph.get_operation_by_name(op_name)\n",
    "    return set(op for out in op.outputs for op in out.consumers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_graph(graph_def) -> dict:\n",
    "    unlikely_output_types = [\n",
    "        'Const', 'Assign',\n",
    "        'NoOp', 'Placeholder',\n",
    "        'Assert', 'switch_t', 'switch_f'\n",
    "    ]\n",
    "    placeholders = dict()\n",
    "    outputs = list()\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():  # pylint: disable=not-context-manager\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "    for node in graph.as_graph_def().node:  # pylint: disable=no-member\n",
    "        if node.op == 'Placeholder':\n",
    "            node_dict = dict()\n",
    "            node_dict['type'] = tf.DType(node.attr['dtype'].type).name\n",
    "            new_shape = tf.TensorShape(node.attr['shape'].shape)\n",
    "            node_dict['shape'] = str(new_shape).replace(' ', '').replace('?', '-1')\n",
    "            placeholders[node.name] = node_dict\n",
    "        if len(children(node.name, graph)) == 0:\n",
    "            if node.op not in unlikely_output_types and \\\n",
    "                node.name.split('/')[-1] not in unlikely_output_types:\n",
    "                outputs.append(node.name)\n",
    "    result = dict()\n",
    "    result['inputs'] = placeholders\n",
    "    result['outputs'] = outputs\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_refs(graph: tf.Graph, input_data: dict):\n",
    "    \"\"\"\n",
    "    Return TensorFlow model reference results.\n",
    "    \"\"\"\n",
    "    log.info(\"Running inference with tensorflow ...\")\n",
    "    feed_dict = {}\n",
    "    summary_info = summarize_graph(graph.as_graph_def())\n",
    "    input_layers, output_layers = list(summary_info['inputs'].keys()), summary_info['outputs']\n",
    "\n",
    "    data_keys = [key for key in input_data.keys()]\n",
    "    if sorted(input_layers) != sorted(data_keys):\n",
    "        raise ValueError('input data keys: {0} do not match input '\n",
    "                         'layers of network: {1}'.format(data_keys, input_layers))\n",
    "\n",
    "    for input_layer_name in input_layers:\n",
    "        tensor = graph.get_tensor_by_name(input_layer_name + ':0')\n",
    "        feed_dict[tensor] = input_data[input_layer_name]\n",
    "    output_tensors = []\n",
    "    for name in output_layers:\n",
    "        tensor = graph.get_tensor_by_name(name + ':0')\n",
    "        output_tensors.append(tensor)\n",
    "\n",
    "    log.info(\"Running tf.Session\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # force inference on CPU\n",
    "    with graph.as_default():\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            inference_start = time.time()\n",
    "            outputs = session.run(output_tensors, feed_dict=feed_dict)\n",
    "            inference_end = time.time()\n",
    "    res = dict(zip(output_layers, outputs))\n",
    "    log.info(\"TensorFlow reference collected successfully\\n\")\n",
    "    return res, inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def parse_od_output(data: dict):\n",
    "    predictions = []\n",
    "    num_batches = len(data['detection_boxes'])\n",
    "    target_layers = ['num_detections', 'detection_classes',\n",
    "                     'detection_scores', 'detection_boxes']\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        predictions.append([])\n",
    "        num_detections = int(data['num_detections'][b])\n",
    "        detection_classes = data['detection_classes'][b]\n",
    "        detection_scores = data['detection_scores'][b]\n",
    "        detection_boxes = data['detection_boxes'][b]\n",
    "        for i in range(num_detections):\n",
    "            obj = [\n",
    "                b, detection_classes[i], detection_scores[i],\n",
    "                detection_boxes[i][1], detection_boxes[i][0],\n",
    "                detection_boxes[i][3], detection_boxes[i][2]\n",
    "            ]\n",
    "            predictions[b].append(obj)\n",
    "    predictions = np.asarray(predictions)\n",
    "    new_shape = (1, 1, predictions.shape[0] * predictions.shape[1], predictions.shape[2])\n",
    "    predictions = np.reshape(predictions, newshape=new_shape)\n",
    "    parsed_data = {'tf_detections': predictions}\n",
    "    for layer, blob in data.items():\n",
    "        if layer not in target_layers:\n",
    "            parsed_data.update({layer: blob})\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def tf_main(path_to_model: str, path_to_original_image: str, batch: int = 1):\n",
    "    \"\"\"\n",
    "    Entrypoint for inferencing with TensorFlow\n",
    "    \"\"\"\n",
    "    log.info('COMMON: image preprocessing')\n",
    "    width = 300\n",
    "    resized_image = read_resize_image(path_to_original_image, width, width)\n",
    "    reshaped_image = np.reshape(resized_image, (width, width, 3))\n",
    "    batched_image = np.array([reshaped_image for _ in range(batch)])\n",
    "    \n",
    "    log.info('Current shape: {}'.format(batched_image.shape))\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Loading a model with TensorFLow')\n",
    "    graph = load_graph(path_to_model)\n",
    "\n",
    "    input_data = {\n",
    "        'image_tensor': batched_image,\n",
    "    }\n",
    "\n",
    "    raw_results, delta = get_refs(graph, input_data)\n",
    "    log.info('TENSORFLOW SPECIFIC: Plain inference finished')\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Post processing started')\n",
    "    processed_results = parse_od_output(raw_results)\n",
    "    log.info('TENSORFLOW SPECIFIC: Post processing finished')\n",
    "\n",
    "    return processed_results['tf_detections'], delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def ie_main(path_to_model_xml: str, path_to_model_bin: str, path_to_original_image: str, device='CPU', batch=1):\n",
    "    # First create Network (Note you need to provide model in IR previously converted with Model Optimizer)\n",
    "    log.info(\"Reading IR...\")\n",
    "    net = IENetwork(model=path_to_model_xml, weights=path_to_model_bin)\n",
    "\n",
    "    # Now let's create IECore() entity \n",
    "    log.info(\"Creating Inference Engine Core\")   \n",
    "    ie = IECore()\n",
    "\n",
    "\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    net.reshape({input_blob: (batch, c, h, w)})\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    \n",
    "    log.info('COMMON: image preprocessing')\n",
    "    image = read_resize_image(path_to_original_image, h, w)\n",
    "    # Now we load Network to plugin\n",
    "    log.info(\"Loading IR to the plugin...\")\n",
    "    exec_net = ie.load_network(network=net, device_name=device, num_requests=2)\n",
    "\n",
    "    del net\n",
    "\n",
    "    labels_map = None\n",
    "    \n",
    "    # Read and pre-process input image\n",
    "    image = image[..., ::-1]\n",
    "    in_frame = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    batched_frame = np.array([in_frame for _ in range(batch)])\n",
    "    log.info('Current shape: {}'.format(batched_frame.sTF_RESULT_IMAGEhape))\n",
    "\n",
    "    # Now we run inference on target device\n",
    "    inference_start = time.time()\n",
    "    res = exec_net.infer(inputs={input_blob: batched_frame})\n",
    "    inference_end = time.time()\n",
    "\n",
    "    log.info('INFERENCE ENGINE SPECIFIC: no post processing')\n",
    "\n",
    "    return res[out_blob], inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "\n",
    "NUM_RUNS = 1\n",
    "BATCH = 1\n",
    "\n",
    "DATA = os.path.join('.', 'data')\n",
    "\n",
    "IMAGE = os.path.join(DATA, 'images', 'input', 'dog.jpg')\n",
    "\n",
    "SSD_ASSETS = os.path.join(DATA, 'public', 'ssd_mobilenet_v2_coco')\n",
    "\n",
    "TF_MODEL = os.path.join(SSD_ASSETS, 'ssd_mobilenet_v2_coco_2018_03_29', 'frozen_inference_graph.pb')\n",
    "TF_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'tensorflow_output.png')\n",
    "\n",
    "IE_MODEL_FP32_XML = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_FP32_BIN = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "IE_MODEL_DEFAULT_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_DEFAULT_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "IE_MODEL_AA_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_AA_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "\n",
    "IE_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'inference_engine_output.png')\n",
    "\n",
    "OPENVINO = os.getenv('INTEL_OPENVINO_DIR')\n",
    "if not OPENVINO:\n",
    "    print('Please, install OpenVINO and initialize the environment')\n",
    "    sys.exit(1)\n",
    "\n",
    "COMBO_RESULT_IMAGE = os.path.join(DATA, 'images', 'output', 'combo_output.png')\n",
    "\n",
    "PERFORMANCE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def ie_inference(xml:str, bin:str, device:str, postfix: str):\n",
    "    name = '{f} {p} on {d}'.format(f='IE', p=postfix, d=device)\n",
    "\n",
    "    ie_fps_collected = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        predictions, inf_time = ie_main(xml, bin,\n",
    "                                        IMAGE,\n",
    "                                        device,\n",
    "                                        batch=BATCH)\n",
    "        ie_fps = 1 / inf_time\n",
    "        ie_fps_collected.append(ie_fps)\n",
    "\n",
    "    ie_avg_fps = (sum(ie_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "\n",
    "    PERFORMANCE[name] = ie_avg_fps\n",
    "\n",
    "    log.info('{} FPS: {}'.format(name, ie_avg_fps))\n",
    "\n",
    "    draw_image(IMAGE, predictions, IE_RESULT_IMAGE, color=(0, 0, 255))\n",
    "    \n",
    "    return ie_avg_fps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "framework = 'TF'\n",
    "device = 'CPU'\n",
    "name = '{f} on {d}'.format(f=framework, d=device)\n",
    "\n",
    "tf_fps_collected = []\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    predictions, inf_time = tf_main(TF_MODEL, \n",
    "                                    IMAGE,\n",
    "                                    batch=BATCH)\n",
    "    tf_fps = 1 / inf_time\n",
    "    tf_fps_collected.append(tf_fps)\n",
    "\n",
    "tf_avg_fps = (sum(tf_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "\n",
    "PERFORMANCE[name] = tf_avg_fps\n",
    "\n",
    "log.info('{} FPS: {}'.format(name, tf_avg_fps))\n",
    "draw_image(IMAGE, predictions, TF_RESULT_IMAGE, color=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy checker\n",
    "\n",
    "![](pictures/accuracy_check.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "models:\n",
    "    - name:  ssd_mobilenet_v1_coco\n",
    "      launchers:\n",
    "        - framework: dlsdk\n",
    "          tags:\n",
    "            - FP32\n",
    "          model: data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml\n",
    "          weights: data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.bin\n",
    "          adapter: ssd\n",
    "          device: CPU\n",
    "  \n",
    "      datasets:\n",
    "        - name: ms_coco_detection_91_classes\n",
    "          data_source: data/datasets/COCO200\n",
    "          annotation_conversion: \n",
    "              annotation_file: data/datasets/COCO200/instances_val2017_200pictures.json\n",
    "              has_background: True\n",
    "              use_full_label_map: True\n",
    "              converter: mscoco_detection\n",
    "          preprocessing:\n",
    "            - type: resize\n",
    "              size: 300\n",
    "          postprocessing:\n",
    "            - type: resize_prediction_boxes\n",
    "          metrics:\n",
    "            - type: coco_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/accuracy_checker_config_tf.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_FP32_XML, IE_MODEL_FP32_BIN, device, '')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantization\n",
    "\n",
    "![](pictures/quantization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Post-training optimization toolkit\n",
    "![](pictures/pot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two types of optimization\n",
    "\n",
    "![](pictures/pot_algos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermediate representation after quantizing\n",
    "\n",
    "![](pictures/mo_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/default/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/default \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_DEFAULT_INT8_XML, IE_MODEL_DEFAULT_INT8_BIN, device, 'INT8 D')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/default/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/accuracy_aware/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/acuracy_aware \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_inference(IE_MODEL_AA_INT8_XML, IE_MODEL_AA_INT8_BIN, device, 'INT8 AA')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tf_avg_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!accuracy_check -c data/configs/accuracy_aware/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/samples/cpp/build/intel64/Release/benchmark_app -m workshop/data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipywebrtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read/write video, work with images\n",
    "import cv2\n",
    "\n",
    "# Inference\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "# Show videos in the notebook\n",
    "from ipywidgets import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_PATH_XML = 'data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml'\n",
    "MODEL_PATH_BIN = 'data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.bin'\n",
    "DEVICE = 'CPU'\n",
    "INPUT_VIDEO = 'practice/data/artyom.MP4'\n",
    "OUTPUT_VIDEO = 'practice/data/out_artyom.MP4'\n",
    "LABELS_PATH = 'practice/data/coco_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Video.from_file(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def prapare_out_video_stream(input_video_stream):\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    return cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'X264'), 20, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create object of IECore. \n",
    "# This class represents an Inference Engine entity \n",
    "# and allows you to manipulate with plugins using unified interfaces\n",
    "ie = IECore()\n",
    "\n",
    "# Load network as Intermediate Representation \n",
    "# The IENetwork class contains the information about the network model read from Intermediate Representation\n",
    "# and allows you to manipulate with some model parameters such as layers affinity and output layers\n",
    "net = IENetwork(model=MODEL_PATH_XML, weights=MODEL_PATH_BIN)\n",
    "\n",
    "# Get names of input layers of the network\n",
    "input_blob = next(iter(net.inputs))\n",
    "\n",
    "print('Input layer of the network is {}'.format(input_blob))\n",
    "\n",
    "# Get shape (dimensions) of the input layer of the network\n",
    "# n - number of batch\n",
    "# c - number of an input image channels (usualy 3 - R, G and B) \n",
    "# h - height\n",
    "# w - width\n",
    "n, c, h, w = net.inputs[input_blob].shape\n",
    "\n",
    "print('Input shape of the network is [{}, {}, {}, {}]'.format(n, c, h, w))\n",
    "\n",
    "# Get names of output layers of the network\n",
    "out_blob = next(iter(net.outputs))\n",
    "\n",
    "print('Output layer of the network is {}'.format(out_blob))\n",
    "\n",
    "# Load names of COCO classes from the file \n",
    "with open(LABELS_PATH, 'r') as f:\n",
    "    labels_map = [x.strip() for x in f]\n",
    "\n",
    "# Load the network to the device\n",
    "# The load_network function returns an object of ExecutableNetwork\n",
    "# This class represents a network instance loaded to plugin and ready for inference\n",
    "exec_net = ie.load_network(network=net, num_requests=2, device_name=DEVICE)\n",
    "\n",
    "\n",
    "# Open an input video\n",
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)\n",
    "\n",
    "# Create an output video stream\n",
    "out = prapare_out_video_stream(input_video_stream)\n",
    "\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "\n",
    "cur_request_id = 0\n",
    "next_request_id = 1\n",
    "\n",
    "# Do loop ny input video\n",
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    ret, frame = input_video_stream.read()\n",
    "    # Check if video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if video is over\n",
    "        break \n",
    "    # Get height and width of the frame\n",
    "    frame_h, frame_w = frame.shape[:2]\n",
    "    \n",
    "    # Resize the frame to network's input \n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    \n",
    "    # Change data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to network's input \n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    # Prepare data for network.\n",
    "    # This must be a dictionary: \n",
    "    #   key - name of the input layer\n",
    "    #   value - input data (the prepared frame)  \n",
    "    feed_dict[input_blob] = in_frame\n",
    "    \n",
    "    # Start Asynchronous Inference.\n",
    "    # We must set request_id - number or identificator of Inference Request\n",
    "    # and input data - the dictionary\n",
    "    exec_net.start_async(request_id=cur_request_id, inputs=feed_dict)\n",
    "    \n",
    "    # Wait the inference request until Inference Engine finished the inference of the request\n",
    "    if exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "        # Read result of the inference from the out layer of the execution network \n",
    "        inference_request_result = exec_net.requests[cur_request_id].outputs[out_blob]\n",
    "        \n",
    "        # Iterate by all found objects\n",
    "        for obj in inference_request_result[0][0]:\n",
    "            # Draw only objects when probability more than specified threshold\n",
    "            if obj[2] > 0.5:\n",
    "                # Get coordinates of the found object\n",
    "                # and scale it to the original size of the frame\n",
    "                xmin = int(obj[3] * frame_w)\n",
    "                ymin = int(obj[4] * frame_h)\n",
    "                xmax = int(obj[5] * frame_w)\n",
    "                ymax = int(obj[6] * frame_h)\n",
    "                \n",
    "                # Get class ID of the found object\n",
    "                class_id = int(obj[1])\n",
    "                \n",
    "                # Get confidence for the found object.\n",
    "                confidence = round(obj[2] * 100, 1)\n",
    "                \n",
    "                # Draw box and label\n",
    "                color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                \n",
    "                # Get label of the class\n",
    "                label = labels_map[class_id]\n",
    "                \n",
    "                # Create titel of the object\n",
    "                text = '{}: {}% '.format(label, confidence)\n",
    "                \n",
    "                # Put the titel to the frame\n",
    "                cv2.putText(frame, text, (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n",
    "        \n",
    "    # Write the result frame to the out stream\n",
    "    out.write(frame)\n",
    "\n",
    "# Save result video\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Video.from_file(OUTPUT_VIDEO)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
